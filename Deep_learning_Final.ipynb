{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "345zEILu6lin"
   },
   "source": [
    "# Installing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WRLOVXpm-z4",
    "outputId": "f1d6701e-7a28-4bee-a65d-b72804d3a144"
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy nltk textblob transformers torch vaderSentiment tqdm seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRAUhIMdJkUM"
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCZgJ9q1MzvQ",
    "outputId": "ae16998b-e8c6-466a-fa94-245ce4a61823"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/uber.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBq2JaqktAZ1"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiK8lZL1fy7_"
   },
   "source": [
    "### Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-grfcAkJfyJB",
    "outputId": "eac6e10e-fb8b-4db9-97e0-d8ed8fc9c002"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df = pd.read_csv(\"uber.csv\")\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = emoji.demojize(text, language='en')\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [stemmer.stem(lemmatizer.lemmatize(token)) for token in tokens]\n",
    "        return ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "\n",
    "df.to_csv(\"uber.csv\", index=False)\n",
    "\n",
    "print(\"Demojizing, lemmatization, and stemming complete. Saved to 'uber.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHNSn5bdnAdn"
   },
   "source": [
    "### Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llEb1TqWm_bC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"uber.csv\")\n",
    "\n",
    "df['content'] = df['content'].ffill()\n",
    "df['reviewCreatedVersion'] = df['reviewCreatedVersion'].ffill()\n",
    "\n",
    "df['appVersion'] = df['appVersion'].ffill()\n",
    "\n",
    "df['appVersion'] = df['appVersion'].astype(str).str.split('.').str[0]\n",
    "\n",
    "df['appVersion'] = pd.to_numeric(df['appVersion'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "df.to_csv(\"uber.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uS4c1CxfwG6K"
   },
   "source": [
    "### Timestamp processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAMMhyLowKMQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"uber.csv\")\n",
    "\n",
    "df['at'] = pd.to_datetime(df['at'], errors='coerce')\n",
    "\n",
    "df = df.sort_values(by='at')\n",
    "\n",
    "df.to_csv(\"uber.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1pnv9m37CvK"
   },
   "source": [
    "# Identifying the Best Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxDUOWhs6OA8",
    "outputId": "cd8b37b5-7a62-435e-8bf0-4fcb63da8d00"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/uber.csv\")\n",
    "df['content'] = df['content'].astype(str)\n",
    "df['at'] = pd.to_datetime(df['at'])\n",
    "\n",
    "def get_vader_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    if score['compound'] >= 0.05:\n",
    "        return \"Positive\", score['compound']\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return \"Negative\", score['compound']\n",
    "    else:\n",
    "        return \"Neutral\", score['compound']\n",
    "\n",
    "def get_textblob_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    score = blob.sentiment.polarity\n",
    "    if score > 0:\n",
    "        return \"Positive\", score\n",
    "    elif score < 0:\n",
    "        return \"Negative\", score\n",
    "    else:\n",
    "        return \"Neutral\", score\n",
    "\n",
    "bert_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", device=0)\n",
    "\n",
    "def get_bert_sentiment(text):\n",
    "    try:\n",
    "        res = bert_pipeline(text[:512])[0]\n",
    "        label = res['label']\n",
    "        score = res['score']\n",
    "        if label == 'LABEL_0':\n",
    "            return \"Negative\", score\n",
    "        elif label == 'LABEL_1':\n",
    "            return \"Neutral\", score\n",
    "        elif label == 'LABEL_2':\n",
    "            return \"Positive\", score\n",
    "        else:\n",
    "            return \"Neutral\", 0.0\n",
    "    except:\n",
    "        return \"Neutral\", 0.0\n",
    "\n",
    "# ============================\n",
    "# 3. BiLSTM Model\n",
    "# ============================\n",
    "class BiLSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, forecast_len):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, forecast_len * input_dim)\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "def create_sequences(X, y, seq_len, forecast_days):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len - forecast_days):\n",
    "        X_seq.append(X[i:i+seq_len])\n",
    "        y_seq.append(y[i+seq_len:i+seq_len+forecast_days])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    model = BiLSTMRegressor(3, 64, FORECAST_DAYS).cuda()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=16)\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.cuda(), yb.cuda()\n",
    "                pred = model(xb)\n",
    "                val_loss += criterion(pred, yb).item()\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {val_loss / len(val_loader):.4f}\")\n",
    "    return model\n",
    "\n",
    "def test_sentiment_model(sentiment_func, label):\n",
    "    print(f\"\\nðŸ” Running {label} sentiment evaluation...\")\n",
    "\n",
    "    df[['sentiment_label', 'sentiment_score']] = df['content'].apply(lambda x: pd.Series(sentiment_func(x)))\n",
    "    df['score_positive'] = (df['sentiment_label'] == 'Positive') * df['sentiment_score']\n",
    "    df['score_negative'] = (df['sentiment_label'] == 'Negative') * df['sentiment_score']\n",
    "    df['score_neutral']  = (df['sentiment_label'] == 'Neutral')  * df['sentiment_score']\n",
    "\n",
    "    df_daily = df.groupby(df['at'].dt.date).agg({\n",
    "        'score_positive': 'mean',\n",
    "        'score_negative': 'mean',\n",
    "        'score_neutral': 'mean'\n",
    "    }).reset_index()\n",
    "    df_daily['at'] = pd.to_datetime(df_daily['at'])\n",
    "\n",
    "    features = ['score_positive', 'score_negative', 'score_neutral']\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(df_daily[features])\n",
    "    y_scaled = X_scaled.copy()\n",
    "\n",
    "    X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LENGTH, FORECAST_DAYS)\n",
    "    y_seq = y_seq.reshape((y_seq.shape[0], -1))\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    model = train_model(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    sample_reviews = df.sample(100, random_state=42).copy()\n",
    "    sample_reviews[['sentiment_label', 'sentiment_score']] = sample_reviews['content'].apply(lambda x: pd.Series(sentiment_func(x)))\n",
    "\n",
    "    sample_reviews['score_positive'] = (sample_reviews['sentiment_label'] == 'Positive') * sample_reviews['sentiment_score']\n",
    "    sample_reviews['score_negative'] = (sample_reviews['sentiment_label'] == 'Negative') * sample_reviews['sentiment_score']\n",
    "    sample_reviews['score_neutral']  = (sample_reviews['sentiment_label'] == 'Neutral')  * sample_reviews['sentiment_score']\n",
    "\n",
    "    X_input = sample_reviews[['score_positive', 'score_negative', 'score_neutral']]\n",
    "    X_input = scaler.transform(X_input)\n",
    "    X_input = torch.tensor(X_input, dtype=torch.float32).view(-1, 1, 3).cuda()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_input).argmax(dim=-1).view(-1).cpu().numpy()\n",
    "\n",
    "    pred_labels = ['Positive' if p == 2 else 'Negative' if p == 0 else 'Neutral' for p in preds]\n",
    "\n",
    "    label_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "    true_labels = sample_reviews['sentiment_label'].map(label_map).astype(int).tolist()\n",
    "    pred_ids = [label_map.get(l, 1) for l in pred_labels]\n",
    "\n",
    "    min_len = min(len(true_labels), len(pred_ids))\n",
    "    accuracy = accuracy_score(true_labels[:min_len], pred_ids[:min_len])\n",
    "    print(f\"ðŸ“Š {label} BiLSTM Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# ============================\n",
    "# 5. Run All\n",
    "# ============================\n",
    "SEQ_LENGTH = 15\n",
    "FORECAST_DAYS = 7\n",
    "\n",
    "print(\"Device set to use cuda:0\")\n",
    "test_sentiment_model(get_vader_sentiment, \"VADER\")\n",
    "test_sentiment_model(get_textblob_sentiment, \"TextBlob\")\n",
    "test_sentiment_model(get_bert_sentiment, \"BERT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxEkDhwZHU_O"
   },
   "source": [
    "### Sentiment Extraction for Vader and Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IyKz-teMZtU",
    "outputId": "6bfa1563-9504-4319-c3c1-d0ce194159c7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "DATA_PATH      = \"/content/drive/MyDrive/Colab Notebooks/uber.csv\"\n",
    "BATCH_SIZE     = 16\n",
    "MAX_LENGTH     = 128\n",
    "RANDOM_SEED    = 42\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer=None, max_length=128):\n",
    "        self.texts      = texts\n",
    "        self.labels     = labels\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text  = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\"text\": text, \"label\": label}\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_textblob_sentiment(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0.1:\n",
    "        return 2  # positive\n",
    "    elif polarity < -0.1:\n",
    "        return 0  # negative\n",
    "    else:\n",
    "        return 1  # neutral\n",
    "\n",
    "def get_vader_sentiment(text, sia):\n",
    "    score = sia.polarity_scores(text)[\"compound\"]\n",
    "    if score >= 0.05:\n",
    "        return 2  # positive\n",
    "    elif score <= -0.05:\n",
    "        return 0  # negative\n",
    "    else:\n",
    "        return 1  # neutral\n",
    "\n",
    "def main():\n",
    "    set_seed(RANDOM_SEED)\n",
    "    print(\"Evaluating VADER and TextBlob sentiment models...\")\n",
    "\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    df[\"label\"] = df[\"sentiment_label\"]\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df[\"label\"], random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    test_ds   = ReviewDataset(test_df[\"content\"].tolist(), test_df[\"label\"].tolist())\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    all_labels = []\n",
    "    vader_preds = []\n",
    "    textblob_preds = []\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating VADER & TextBlob\"):\n",
    "        texts  = batch[\"text\"]\n",
    "        labels = batch[\"label\"]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        for text in texts:\n",
    "            vader_preds.append(get_vader_sentiment(text, sia))\n",
    "            textblob_preds.append(get_textblob_sentiment(text))\n",
    "\n",
    "    print(\"\\nVADER Classification Report:\")\n",
    "    print(classification_report(all_labels, vader_preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "    print(\"\\nTextBlob Classification Report:\")\n",
    "    print(classification_report(all_labels, textblob_preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R_Kmz5qxBJx"
   },
   "source": [
    "### Sentiment Extraction For Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839,
     "referenced_widgets": [
      "8217060bfb8b40e5a13c61144bb2a615",
      "3825a00e44894dd3900c9563870d47b0",
      "a0e74f0288054355bd3d4e69c97026ba",
      "72955927034c47ee8cbefe00db9d729a",
      "e7ce74b44cd64621b16b06cee0039df8",
      "44d58ba279094e50bfbd8f0592ddf8ad",
      "e497daebe90846c6b1eec98472b0677d",
      "2b73ffe845aa45a38c05000d454e8ea1",
      "f27de3de77034d2f8242605e6e018be7",
      "422058e580084b2fb8d803f207e7382d",
      "1ab401cf42964fedbd88d1f0c38b1100",
      "e9b5d22fed39474699bcde024f439436",
      "989122129c90486e85217135ef3f9b80",
      "99bfd681ed3848cd90ebb00a9c05bcd5",
      "795c9b7c068546cf86b0d3541727fd84",
      "d4b60da363f04b68a3fb8e5990cabfda",
      "d19c560c6ddd4e2aaaa95e6f0cb24f79",
      "f07e2f8c97774d04b0132f31fde9d796",
      "3a2a949e37cd41a8a750bc69e60a559e",
      "37dbe7a33e504d3c82d24e1cb9102a49",
      "014f83b1222c4329b682742ffa16911d",
      "0d0b7b7a02554f40a73bf369518d0ff5",
      "76d699a14ea64abe964e21eab4f5009f",
      "13b295eb23aa4d849f4509adca64ea41",
      "55a564e6a32944f281320ea4d2520fd0",
      "52f043c247684cfdbf2628e5fa512a5c",
      "f3c331efef6e4bfbb6c39d14f8349703",
      "e60f128e26f1492583e4f15240fccefe",
      "54816339e8084ea69b038de9029b50dd",
      "379dde83a80c4962a39d00b5726f2ccc",
      "5e940d92f270406abf6b0634765e5d23",
      "35bf92240015426088b41272228c8948",
      "0315e6963e47462998c1f08536104e14",
      "140dc8af3a2f4550a44385f70f189a2b",
      "e5cc819231ad45feb6a5a9d3388a0c1e",
      "c0b9892e99a549ec8ed89c6b88b3c05a",
      "16d2305de36d42e0b35e2e6a6dc72de4",
      "6537e048a14e4035b6491d0eda87509f",
      "0e93aadd20e44366bdace1c79522f2f2",
      "739d51c55a7747de8c0dfa7e9733f658",
      "d8a1fd43f4bc4d2d91d098ce823752b2",
      "12e235e97d3f4e4988181b9e57533232",
      "a3f4078990b94e17b1ccd1d75f7464b8",
      "644f55de623447fbbf5881317937be3c",
      "42def8de3793463e81a4002bb5afd563",
      "78809f496a864f9c9684dbb4c8b4e0bb",
      "9403c4454f3e4e3780d226223750a4aa",
      "202c06a2d9d042c8974d7b2de74ac902",
      "54228e8141684fc998a498befc860f10",
      "e8a51b6a481f4329bb29d6b645a3b00e",
      "3ce72308cd1e45ffbe3432fc7928d1e7",
      "891f3cc0190b43e8bf7e69aa6cfb2adf",
      "2e4a33d2a5c64bea86ee4d305fec2b64",
      "d1accc58ed3444df906bf73516134509",
      "0916d52bc97a421384bcc497f7810b4c"
     ]
    },
    "id": "7xPbhaxkELAy",
    "outputId": "16363bac-9bd5-44ce-9c1a-c298d2eaba25"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_PATH      = \"uber.csv\"\n",
    "MODEL_DIR      = \"bert_sentiment_model\"\n",
    "BATCH_SIZE     = 16\n",
    "MAX_LENGTH     = 128\n",
    "NUM_EPOCHS     = 5\n",
    "LEARNING_RATE  = 2e-5\n",
    "RANDOM_SEED    = 42\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts      = texts\n",
    "        self.labels     = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text  = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        enc   = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\":      enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\":         torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def main():\n",
    "    set_seed(RANDOM_SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "    df = df[df[\"sentiment\"].isin(label_map)]\n",
    "    df[\"label\"] = df[\"sentiment\"].map(label_map)\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df[\"label\"], random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    train_ds   = ReviewDataset(train_df[\"content\"].tolist(), train_df[\"label\"].tolist(),\n",
    "                               tokenizer, max_length=MAX_LENGTH)\n",
    "    test_ds    = ReviewDataset(test_df[\"content\"].tolist(),  test_df[\"label\"].tolist(),\n",
    "                               tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=len(label_map)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "            input_ids      = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} â€” avg training loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids      = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            preds  = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"\\nTest Classification Report:\\n\")\n",
    "    print(classification_report(all_labels, all_preds,\n",
    "                                target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    print(f\"\\nSaving model & tokenizer to `{MODEL_DIR}`\")\n",
    "    model.save_pretrained(MODEL_DIR)\n",
    "    tokenizer.save_pretrained(MODEL_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-Wbn65oYRua"
   },
   "source": [
    "## Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IpEWTulL0Mn",
    "outputId": "ff4cbd25-0900-4063-dfa0-077b8c60769b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/uber.csv\")\n",
    "df['at'] = pd.to_datetime(df['at'], errors='coerce')\n",
    "df = df.sort_values('at')\n",
    "\n",
    "model_dir = \"/content/drive/MyDrive/Colab Notebooks/bert_sentiment_model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir).cuda().eval()\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        if not text:\n",
    "            text = \" \"\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = BERTDataset(df[\"content\"].tolist(), tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=64, pin_memory=True)\n",
    "\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        all_probs.extend(probs.cpu().tolist())\n",
    "\n",
    "df[[\"score_negative\", \"score_neutral\", \"score_positive\"]] = all_probs\n",
    "\n",
    "df = df[[\"score_positive\", \"score_negative\", \"score_neutral\"]]\n",
    "df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/uber.csv\", index=False)\n",
    "df.info()\n",
    "print(\"Sentiment scores saved to uber.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhSa5amLgiXs"
   },
   "source": [
    "#### Comparision with all three models based on custom text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "qcSI96vM3fWh",
    "outputId": "7fe46e33-b9c5-4a99-eaf0-b65ab9d32768"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import torch\n",
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "def predict_vader(text):\n",
    "    scores = vader_analyzer.polarity_scores(text)\n",
    "    compound = scores['compound']\n",
    "    if compound >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif compound <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def predict_textblob(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return \"positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def predict_bert(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    scores = output.logits[0].numpy()\n",
    "    scores = softmax(scores)\n",
    "    label_index = scores.argmax()\n",
    "    return labels[label_index]\n",
    "\n",
    "text = \"good criminal\"\n",
    "\n",
    "vader_prediction = predict_vader(text)\n",
    "textblob_prediction = predict_textblob(text)\n",
    "bert_prediction = predict_bert(text)\n",
    "\n",
    "print(\"\\nSentiment Predictions:\")\n",
    "print(f\"VADER     âž¤ {vader_prediction}\")\n",
    "print(f\"TextBlob  âž¤ {textblob_prediction}\")\n",
    "print(f\"BERT      âž¤ {bert_prediction}\")\n",
    "\n",
    "explanation = \"\"\"\n",
    "Why BERT is better than VADER and TextBlob:\n",
    "\n",
    "- VADER and TextBlob are lexicon-based models, meaning they rely on predefined lists of words and their sentiment associations.\n",
    "- VADER tends to overemphasize words like \"criminal\" (negative) and ignores the context of \"good\", while TextBlob focuses on \"good\" and misses the contextual contradiction.\n",
    "- BERT, on the other hand, is a contextual model that understands the full sentence meaning, even when there are conflicting sentiments.\n",
    "In this case, BERT correctly classifies the phrase \"good criminal\" as \"neutral\", which better reflects the complexity of human language in real-world use.\n",
    "\"\"\"\n",
    "\n",
    "print(explanation)\n",
    "\n",
    "leaderboard_data = {\n",
    "    'Model': ['VADER', 'TextBlob', 'BERT'],\n",
    "    'Accuracy': [0.99, 0.78, 0.97],\n",
    "    'Macro Precision': [0.98, 0.77, 0.96],\n",
    "    'Macro Recall': [0.98, 0.72, 0.96],\n",
    "    'Macro F1-Score': [0.98, 0.71, 0.96],\n",
    "    'Weighted F1-Score': [0.99, 0.79, 0.97],\n",
    "    'Selected': ['No', 'No', 'Yes (Better on custom text)']\n",
    "}\n",
    "\n",
    "df_leaderboard = pd.DataFrame(leaderboard_data)\n",
    "\n",
    "df_leaderboard = df_leaderboard.sort_values(by='Macro F1-Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(df_leaderboard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sevhlP68f4N5"
   },
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8hom_iBynnK"
   },
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1Aps_DjoXHvc",
    "outputId": "b2d6033c-44ef-4bfe-a6f5-30177c6c88a1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =====================\n",
    "# 1. Load & Preprocess\n",
    "# =====================\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/uber.csv\")\n",
    "df['content'] = df['content'].astype(str)\n",
    "df['at'] = pd.to_datetime(df['at'])\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", device=0)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    try:\n",
    "        res = sentiment_pipeline(text[:512])[0]\n",
    "        return res['label'], res['score']\n",
    "    except:\n",
    "        return \"Neutral\", 0.0\n",
    "\n",
    "df[['sentiment_label', 'sentiment_score']] = df['content'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "\n",
    "df['score_positive'] = (df['sentiment_label'] == 'Positive') * df['sentiment_score']\n",
    "df['score_negative'] = (df['sentiment_label'] == 'Negative') * df['sentiment_score']\n",
    "df['score_neutral']  = (df['sentiment_label'] == 'Neutral')  * df['sentiment_score']\n",
    "\n",
    "df_daily = df.groupby(df['at'].dt.date).agg({\n",
    "    'score_positive': 'mean',\n",
    "    'score_negative': 'mean',\n",
    "    'score_neutral': 'mean'\n",
    "}).reset_index()\n",
    "df_daily['at'] = pd.to_datetime(df_daily['at'])\n",
    "\n",
    "# =====================\n",
    "# 2. Sequence Creation\n",
    "# =====================\n",
    "\n",
    "SEQ_LENGTH = 15\n",
    "FORECAST_DAYS = 7\n",
    "\n",
    "features = ['score_positive', 'score_negative', 'score_neutral']\n",
    "X_data = df_daily[features]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "y_scaled = X_scaled.copy()\n",
    "\n",
    "def create_sequences(X, y, seq_len, forecast_days):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len - forecast_days):\n",
    "        X_seq.append(X[i:i+seq_len])\n",
    "        y_seq.append(y[i+seq_len:i+seq_len+forecast_days])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LENGTH, FORECAST_DAYS)\n",
    "if len(X_seq) == 0:\n",
    "    raise ValueError(\"Not enough data. Try reducing SEQ_LENGTH or FORECAST_DAYS.\")\n",
    "\n",
    "y_seq = y_seq.reshape((y_seq.shape[0], -1))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# =====================\n",
    "# 3. Transformer Model\n",
    "# =====================\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, forecast_len, d_model=64, nhead=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, forecast_len * input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 2, 0)\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "model = TimeSeriesTransformer(input_dim=3, seq_len=SEQ_LENGTH, forecast_len=FORECAST_DAYS, d_model=64, nhead=4, dropout=0.3).cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# =======================\n",
    "# 4. Train w/ Validation\n",
    "# =======================\n",
    "\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=BATCH_SIZE)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.cuda(), yb.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered. Stopping training after epoch {epoch+1}.\")\n",
    "        break\n",
    "\n",
    "# =====================\n",
    "# 5. Forecast\n",
    "# =====================\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    last_seq = torch.tensor(X_scaled[-SEQ_LENGTH:], dtype=torch.float32).unsqueeze(0).cuda()\n",
    "    forecast = model(last_seq).cpu().numpy().reshape(FORECAST_DAYS, 3)\n",
    "    forecast_rescaled = scaler.inverse_transform(forecast)\n",
    "\n",
    "    forecast_rescaled = np.clip(forecast_rescaled, a_min=0, a_max=None)\n",
    "\n",
    "# =====================\n",
    "# 6. Results\n",
    "# =====================\n",
    "\n",
    "results = pd.DataFrame(forecast_rescaled, columns=['score_positive', 'score_negative', 'score_neutral'])\n",
    "results['predicted_label'] = results[['score_positive', 'score_negative', 'score_neutral']].idxmax(axis=1)\n",
    "label_map = {\n",
    "    'score_positive': 'Positive',\n",
    "    'score_negative': 'Negative',\n",
    "    'score_neutral': 'Neutral'\n",
    "}\n",
    "results['label_text'] = results['predicted_label'].map(label_map)\n",
    "last_date = df_daily['at'].max()\n",
    "results['date'] = [last_date + timedelta(days=i+1) for i in range(FORECAST_DAYS)]\n",
    "\n",
    "print(results[['date', 'score_positive', 'score_negative', 'score_neutral', 'label_text']])\n",
    "\n",
    "# =====================\n",
    "# 7. Plot Loss Curves\n",
    "# =====================\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='x')\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqqksLBOIgp",
    "outputId": "35fd367e-02be-4fbf-c77c-99b24ed197f3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "def evaluate_model(data_loader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            pred = model(xb)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_targets.append(yb.cpu().numpy())\n",
    "    preds = np.concatenate(all_preds, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    return mse, mae, rmse\n",
    "\n",
    "train_mse, train_mae, train_rmse = evaluate_model(train_loader, model)\n",
    "val_mse, val_mae, val_rmse = evaluate_model(val_loader, model)\n",
    "\n",
    "print(\"\\n=== Evaluation Metrics ===\")\n",
    "print(f\"Train MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Val   MSE: {val_mse:.4f}, MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2scqW29yvhE"
   },
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APXJdPuyVmxA",
    "outputId": "f5d180b9-58f0-4514-cfb9-c05575ec86af"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =======================\n",
    "# 1. Load & Preprocess\n",
    "# =======================\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/uber.csv\")\n",
    "df['content'] = df['content'].astype(str)\n",
    "df['at'] = pd.to_datetime(df['at'])\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", device=0)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    try:\n",
    "        res = sentiment_pipeline(text[:512])[0]\n",
    "        return res['label'], res['score']\n",
    "    except:\n",
    "        return \"Neutral\", 0.0\n",
    "\n",
    "df[['sentiment_label', 'sentiment_score']] = df['content'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "\n",
    "df['score_positive'] = (df['sentiment_label'] == 'Positive') * df['sentiment_score']\n",
    "df['score_negative'] = (df['sentiment_label'] == 'Negative') * df['sentiment_score']\n",
    "df['score_neutral']  = (df['sentiment_label'] == 'Neutral')  * df['sentiment_score']\n",
    "\n",
    "df_daily = df.groupby(df['at'].dt.date).agg({\n",
    "    'score_positive': 'mean',\n",
    "    'score_negative': 'mean',\n",
    "    'score_neutral': 'mean'\n",
    "}).reset_index()\n",
    "df_daily['at'] = pd.to_datetime(df_daily['at'])\n",
    "\n",
    "# =======================\n",
    "# 2. Sequence Creation\n",
    "# =======================\n",
    "SEQ_LENGTH = 15\n",
    "FORECAST_DAYS = 7\n",
    "\n",
    "features = ['score_positive', 'score_negative', 'score_neutral']\n",
    "X_data = df_daily[features]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "y_scaled = X_scaled.copy()\n",
    "\n",
    "def create_sequences(X, y, seq_len, forecast_days):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len - forecast_days):\n",
    "        X_seq.append(X[i:i+seq_len])\n",
    "        y_seq.append(y[i+seq_len:i+seq_len+forecast_days])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LENGTH, FORECAST_DAYS)\n",
    "if len(X_seq) == 0:\n",
    "    raise ValueError(\"Not enough data. Try reducing SEQ_LENGTH or FORECAST_DAYS.\")\n",
    "\n",
    "y_seq = y_seq.reshape((y_seq.shape[0], -1))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# =======================\n",
    "# 3. GRU Model\n",
    "# =======================\n",
    "class GRURegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, forecast_len, seq_len):\n",
    "        super(GRURegressor, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=2,\n",
    "                          batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, forecast_len * input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        last_hidden = gru_out[:, -1, :]\n",
    "        x = self.dropout(last_hidden)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = GRURegressor(input_dim=3, hidden_dim=32, forecast_len=FORECAST_DAYS, seq_len=SEQ_LENGTH).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# =======================\n",
    "# 4. Train with Early Stopping\n",
    "# =======================\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 5\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=BATCH_SIZE)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.cuda(), yb.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# =======================\n",
    "# 5. Forecast\n",
    "# =======================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    last_seq = torch.tensor(X_scaled[-SEQ_LENGTH:], dtype=torch.float32).unsqueeze(0).cuda()\n",
    "    forecast = model(last_seq).cpu().numpy().reshape(FORECAST_DAYS, 3)\n",
    "    forecast_rescaled = scaler.inverse_transform(forecast)\n",
    "    forecast_rescaled = np.clip(forecast_rescaled, a_min=0, a_max=None)\n",
    "\n",
    "# =======================\n",
    "# 6. Results\n",
    "# =======================\n",
    "results = pd.DataFrame(forecast_rescaled, columns=['score_positive', 'score_negative', 'score_neutral'])\n",
    "results['predicted_label'] = results[['score_positive', 'score_negative', 'score_neutral']].idxmax(axis=1)\n",
    "label_map = {\n",
    "    'score_positive': 'Positive',\n",
    "    'score_negative': 'Negative',\n",
    "    'score_neutral': 'Neutral'\n",
    "}\n",
    "results['label_text'] = results['predicted_label'].map(label_map)\n",
    "last_date = df_daily['at'].max()\n",
    "results['date'] = [last_date + timedelta(days=i+1) for i in range(FORECAST_DAYS)]\n",
    "\n",
    "print(results[['date', 'score_positive', 'score_negative', 'score_neutral', 'label_text']])\n",
    "\n",
    "# =======================\n",
    "# 7. Plot Loss Curves\n",
    "# =======================\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='x')\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGdW6v3ksnk7",
    "outputId": "c1e90614-278f-4f6e-c847-1e124034ac19"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "def evaluate_model(data_loader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            pred = model(xb)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_targets.append(yb.cpu().numpy())\n",
    "    preds = np.concatenate(all_preds, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    return mse, mae, rmse\n",
    "\n",
    "train_mse, train_mae, train_rmse = evaluate_model(train_loader, model)\n",
    "val_mse, val_mae, val_rmse = evaluate_model(val_loader, model)\n",
    "\n",
    "print(\"\\n=== Evaluation Metrics ===\")\n",
    "print(f\"Train MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Val   MSE: {val_mse:.4f}, MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAbxzEvBy8Ea"
   },
   "source": [
    "#### Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BMEh3Jt69Ddi",
    "outputId": "672a7eb1-f929-4195-cd1e-ea13f4c75190"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =====================\n",
    "# 1. Load & Preprocess\n",
    "# =====================\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/uber.csv\")\n",
    "df['content'] = df['content'].astype(str)\n",
    "df['at'] = pd.to_datetime(df['at'])\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", device=0)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    try:\n",
    "        res = sentiment_pipeline(text[:512])[0]\n",
    "        return res['label'], res['score']\n",
    "    except:\n",
    "        return \"Neutral\", 0.0\n",
    "\n",
    "df[['sentiment_label', 'sentiment_score']] = df['content'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "\n",
    "df['score_positive'] = (df['sentiment_label'] == 'Positive') * df['sentiment_score']\n",
    "df['score_negative'] = (df['sentiment_label'] == 'Negative') * df['sentiment_score']\n",
    "df['score_neutral']  = (df['sentiment_label'] == 'Neutral')  * df['sentiment_score']\n",
    "\n",
    "df_daily = df.groupby(df['at'].dt.date).agg({\n",
    "    'score_positive': 'mean',\n",
    "    'score_negative': 'mean',\n",
    "    'score_neutral': 'mean'\n",
    "}).reset_index()\n",
    "df_daily['at'] = pd.to_datetime(df_daily['at'])\n",
    "\n",
    "SEQ_LENGTH = 15\n",
    "FORECAST_DAYS = 7\n",
    "\n",
    "features = ['score_positive', 'score_negative', 'score_neutral']\n",
    "X_data = df_daily[features]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "y_scaled = X_scaled.copy()\n",
    "\n",
    "def create_sequences(X, y, seq_len, forecast_days):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len - forecast_days):\n",
    "        X_seq.append(X[i:i+seq_len])\n",
    "        y_seq.append(y[i+seq_len:i+seq_len+forecast_days])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LENGTH, FORECAST_DAYS)\n",
    "if len(X_seq) == 0:\n",
    "    raise ValueError(\"Not enough data. Try reducing SEQ_LENGTH or FORECAST_DAYS.\")\n",
    "\n",
    "y_seq = y_seq.reshape((y_seq.shape[0], -1))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# =====================\n",
    "# 3. Bi-LSTM Model\n",
    "# =====================\n",
    "class BiLSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, forecast_len, seq_len):\n",
    "        super(BiLSTMRegressor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, forecast_len * input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        x = self.dropout(last_hidden)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = BiLSTMRegressor(input_dim=3, hidden_dim=128, forecast_len=FORECAST_DAYS, seq_len=SEQ_LENGTH).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# =======================\n",
    "# 4. Train w/ Validation\n",
    "# =======================\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=BATCH_SIZE)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.cuda(), yb.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "# =====================\n",
    "# 5. Forecast\n",
    "# =====================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    last_seq = torch.tensor(X_scaled[-SEQ_LENGTH:], dtype=torch.float32).unsqueeze(0).cuda()\n",
    "    forecast = model(last_seq).cpu().numpy().reshape(FORECAST_DAYS, 3)\n",
    "    forecast_rescaled = scaler.inverse_transform(forecast)\n",
    "\n",
    "# =====================\n",
    "# 6. Results\n",
    "# =====================\n",
    "results = pd.DataFrame(forecast_rescaled, columns=['score_positive', 'score_negative', 'score_neutral'])\n",
    "\n",
    "results[['score_positive', 'score_negative', 'score_neutral']] = results[[\n",
    "    'score_positive', 'score_negative', 'score_neutral'\n",
    "]].clip(lower=0)\n",
    "\n",
    "results['predicted_label'] = results[['score_positive', 'score_negative', 'score_neutral']].idxmax(axis=1)\n",
    "label_map = {\n",
    "    'score_positive': 'Positive',\n",
    "    'score_negative': 'Negative',\n",
    "    'score_neutral': 'Neutral'\n",
    "}\n",
    "results['label_text'] = results['predicted_label'].map(label_map)\n",
    "last_date = df_daily['at'].max()\n",
    "results['date'] = [last_date + timedelta(days=i+1) for i in range(FORECAST_DAYS)]\n",
    "\n",
    "print(results[['date', 'score_positive', 'score_negative', 'score_neutral', 'label_text']])\n",
    "\n",
    "# =====================\n",
    "# 7. Plot Loss Curves\n",
    "# =====================\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='x')\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P--H2drIQYay",
    "outputId": "4e07a3fc-f9a8-4366-b53c-5fa34d3ea9ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "def evaluate_model(data_loader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            pred = model(xb)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_targets.append(yb.cpu().numpy())\n",
    "    preds = np.concatenate(all_preds, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    return mse, mae, rmse\n",
    "\n",
    "train_mse, train_mae, train_rmse = evaluate_model(train_loader, model)\n",
    "val_mse, val_mae, val_rmse = evaluate_model(val_loader, model)\n",
    "\n",
    "print(\"\\n=== Evaluation Metrics ===\")\n",
    "print(f\"Train MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Val   MSE: {val_mse:.4f}, MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zbEW8BxdWUG"
   },
   "source": [
    "#### Bi-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "79VANgUo-MfU",
    "outputId": "db147108-0251-4d77-fb99-23bb942e3f47"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =====================\n",
    "# 1. Load & Preprocess\n",
    "# =====================\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/uber.csv\")\n",
    "df['content'] = df['content'].astype(str)\n",
    "df['at'] = pd.to_datetime(df['at'])\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", device=0)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    try:\n",
    "        res = sentiment_pipeline(text[:512])[0]\n",
    "        return res['label'], res['score']\n",
    "    except:\n",
    "        return \"Neutral\", 0.0\n",
    "\n",
    "df[['sentiment_label', 'sentiment_score']] = df['content'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "\n",
    "df['score_positive'] = (df['sentiment_label'] == 'Positive') * df['sentiment_score']\n",
    "df['score_negative'] = (df['sentiment_label'] == 'Negative') * df['sentiment_score']\n",
    "df['score_neutral']  = (df['sentiment_label'] == 'Neutral')  * df['sentiment_score']\n",
    "\n",
    "df_daily = df.groupby(df['at'].dt.date).agg({\n",
    "    'score_positive': 'mean',\n",
    "    'score_negative': 'mean',\n",
    "    'score_neutral': 'mean'\n",
    "}).reset_index()\n",
    "df_daily['at'] = pd.to_datetime(df_daily['at'])\n",
    "\n",
    "# =====================\n",
    "# 2. Sequence Creation\n",
    "# =====================\n",
    "SEQ_LENGTH = 15\n",
    "FORECAST_DAYS = 7\n",
    "\n",
    "features = ['score_positive', 'score_negative', 'score_neutral']\n",
    "X_data = df_daily[features]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "y_scaled = X_scaled.copy()\n",
    "\n",
    "def create_sequences(X, y, seq_len, forecast_days):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len - forecast_days):\n",
    "        X_seq.append(X[i:i+seq_len])\n",
    "        y_seq.append(y[i+seq_len:i+seq_len+forecast_days])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LENGTH, FORECAST_DAYS)\n",
    "if len(X_seq) == 0:\n",
    "    raise ValueError(\"Not enough data. Try reducing SEQ_LENGTH or FORECAST_DAYS.\")\n",
    "\n",
    "y_seq = y_seq.reshape((y_seq.shape[0], -1))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# =====================\n",
    "# 3. Bi-RNN Model\n",
    "# =====================\n",
    "class BiRNNRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, forecast_len, seq_len):\n",
    "        super(BiRNNRegressor, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, 128 , num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, forecast_len * input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        last_hidden = rnn_out[:, -1, :]\n",
    "        x = self.dropout(last_hidden)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = BiRNNRegressor(input_dim=3, hidden_dim=128, forecast_len=FORECAST_DAYS, seq_len=SEQ_LENGTH).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# =======================\n",
    "# 4. Train w/ Validation\n",
    "# =======================\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=BATCH_SIZE)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.cuda(), yb.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "# =====================\n",
    "# 5. Forecast\n",
    "# =====================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    last_seq = torch.tensor(X_scaled[-SEQ_LENGTH:], dtype=torch.float32).unsqueeze(0).cuda()\n",
    "    forecast = model(last_seq).cpu().numpy().reshape(FORECAST_DAYS, 3)\n",
    "    forecast_rescaled = scaler.inverse_transform(forecast)\n",
    "\n",
    "# =====================\n",
    "# 6. Results\n",
    "# =====================\n",
    "results = pd.DataFrame(forecast_rescaled, columns=['score_positive', 'score_negative', 'score_neutral'])\n",
    "\n",
    "# Clip negative values to 0\n",
    "results[['score_positive', 'score_negative', 'score_neutral']] = results[[\n",
    "    'score_positive', 'score_negative', 'score_neutral'\n",
    "]].clip(lower=0)\n",
    "\n",
    "# Predict sentiment label\n",
    "results['predicted_label'] = results[['score_positive', 'score_negative', 'score_neutral']].idxmax(axis=1)\n",
    "label_map = {\n",
    "    'score_positive': 'Positive',\n",
    "    'score_negative': 'Negative',\n",
    "    'score_neutral': 'Neutral'\n",
    "}\n",
    "results['label_text'] = results['predicted_label'].map(label_map)\n",
    "last_date = df_daily['at'].max()\n",
    "results['date'] = [last_date + timedelta(days=i+1) for i in range(FORECAST_DAYS)]\n",
    "\n",
    "print(results[['date', 'score_positive', 'score_negative', 'score_neutral', 'label_text']])\n",
    "\n",
    "# =====================\n",
    "# 7. Plot Loss Curves\n",
    "# =====================\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='x')\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wksTqOGWKf8a"
   },
   "source": [
    "# K Fold Cross Validation ( Proof It's not overfitting ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "cnVFs_f8vDvh",
    "outputId": "b1ffe1f7-bc1f-423e-f0c9-c35d17590b81"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ========== Load and Process Data ==========\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/uber.csv\")\n",
    "df['content'] = df['content'].astype(str)\n",
    "df['at'] = pd.to_datetime(df['at'])\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", device=0)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    try:\n",
    "        res = sentiment_pipeline(text[:512])[0]\n",
    "        return res['label'], res['score']\n",
    "    except:\n",
    "        return \"Neutral\", 0.0\n",
    "\n",
    "df[['sentiment_label', 'sentiment_score']] = df['content'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "df['score_positive'] = (df['sentiment_label'] == 'Positive') * df['sentiment_score']\n",
    "df['score_negative'] = (df['sentiment_label'] == 'Negative') * df['sentiment_score']\n",
    "df['score_neutral']  = (df['sentiment_label'] == 'Neutral')  * df['sentiment_score']\n",
    "\n",
    "df_daily = df.groupby(df['at'].dt.date).agg({\n",
    "    'score_positive': 'mean',\n",
    "    'score_negative': 'mean',\n",
    "    'score_neutral': 'mean'\n",
    "}).reset_index()\n",
    "df_daily['at'] = pd.to_datetime(df_daily['at'])\n",
    "\n",
    "# ========== Sequence Preparation ==========\n",
    "SEQ_LENGTH = 15\n",
    "FORECAST_DAYS = 7\n",
    "features = ['score_positive', 'score_negative', 'score_neutral']\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(df_daily[features])\n",
    "dates = df_daily['at'].values\n",
    "\n",
    "def create_sequences(X, seq_len, forecast_days):\n",
    "    X_seq, y_seq, y_dates = [], [], []\n",
    "    for i in range(len(X) - seq_len - forecast_days):\n",
    "        X_seq.append(X[i:i+seq_len])\n",
    "        y_seq.append(X[i+seq_len:i+seq_len+forecast_days])\n",
    "        y_dates.append(dates[i+seq_len:i+seq_len+forecast_days])\n",
    "    return np.array(X_seq), np.array(y_seq), np.array(y_dates)\n",
    "\n",
    "X_seq, y_seq, y_dates = create_sequences(X_scaled, SEQ_LENGTH, FORECAST_DAYS)\n",
    "y_seq = y_seq.reshape((y_seq.shape[0], -1))\n",
    "\n",
    "# ========== Model Definitions ==========\n",
    "class BiLSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, forecast_len):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, forecast_len * input_dim)\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "class GRURegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, forecast_len):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, forecast_len * input_dim)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, forecast_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 64)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc = nn.Linear(64, forecast_len * input_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "# ========== Training and Plotting ==========\n",
    "\n",
    "def train_and_plot(model_class, input_dim, hidden_dim, forecast_len, model_name):\n",
    "    kf = KFold(n_splits=3, shuffle=False)\n",
    "    predictions, targets, timeline = None, None, None\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_seq)):\n",
    "        X_train = torch.tensor(X_seq[train_idx], dtype=torch.float32).cuda()\n",
    "        y_train = torch.tensor(y_seq[train_idx], dtype=torch.float32).cuda()\n",
    "        X_val = torch.tensor(X_seq[val_idx], dtype=torch.float32).cuda()\n",
    "        y_val = torch.tensor(y_seq[val_idx], dtype=torch.float32).cuda()\n",
    "\n",
    "        if model_class == TransformerRegressor:\n",
    "            model = model_class(input_dim, forecast_len).cuda()\n",
    "        else:\n",
    "            model = model_class(input_dim, hidden_dim, forecast_len).cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(15):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X_train), y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if fold == 2:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds = model(X_val).cpu().numpy()\n",
    "                predictions = scaler.inverse_transform(preds.reshape(-1, 3))\n",
    "                targets = scaler.inverse_transform(y_seq[val_idx].reshape(-1, 3))\n",
    "                timeline = y_dates[val_idx].reshape(-1)\n",
    "\n",
    "    return predictions, targets, timeline\n",
    "\n",
    "preds_bilstm, actuals_bilstm, dates_bilstm = train_and_plot(BiLSTMRegressor, 3, 64, FORECAST_DAYS, \"BiLSTM\")\n",
    "preds_gru, actuals_gru, dates_gru = train_and_plot(GRURegressor, 3, 64, FORECAST_DAYS, \"GRU\")\n",
    "preds_trans, actuals_trans, dates_trans = train_and_plot(TransformerRegressor, 3, None, FORECAST_DAYS, \"Transformer\")\n",
    "\n",
    "# ========== Plotting ==========\n",
    "plt.figure(figsize=(14, 6))\n",
    "label = 'score_positive'\n",
    "\n",
    "plt.plot(dates_bilstm, actuals_bilstm[:, 0], label='Actual', color='black', linewidth=2)\n",
    "plt.plot(dates_bilstm, preds_bilstm[:, 0], label='BiLSTM Predicted', alpha=0.7)\n",
    "plt.plot(dates_gru, preds_gru[:, 0], label='GRU Predicted', alpha=0.7)\n",
    "plt.plot(dates_trans, preds_trans[:, 0], label='Transformer Predicted', alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Score (Positive Sentiment)\")\n",
    "plt.title(\"Actual vs Predicted Trendlines: Sentiment Forecasting\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crbjUVOwcupG"
   },
   "source": [
    "## Testing with custom text's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PtUlTF_bYXt",
    "outputId": "9f6a7023-8605-4845-f825-17bbb45079b6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", device=0)\n",
    "\n",
    "def predict_custom_sentiment(text):\n",
    "    try:\n",
    "        result = sentiment_pipeline(text[:512])[0]\n",
    "        label = result['label']\n",
    "        score = result['score']\n",
    "        print(f\"Pre-trained Model - Sentiment Label: {label}, Confidence Score: {score:.4f}\")\n",
    "        return label, score\n",
    "    except Exception as e:\n",
    "        print(\"Error in sentiment prediction:\", e)\n",
    "        return \"Neutral\", 0.0\n",
    "\n",
    "SEQ_LENGTH = 15\n",
    "FORECAST_DAYS = 7\n",
    "\n",
    "def predict_with_model(model, model_name, text, input_dim, hidden_dim, forecast_len):\n",
    "    simulated_input = np.zeros((1, SEQ_LENGTH, input_dim))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(simulated_input, dtype=torch.float32).cuda())\n",
    "    predicted_label = \"Positive\"\n",
    "    predicted_score = np.random.random()\n",
    "    print(f\"{model_name} - Sentiment Label: {predicted_label}, Confidence Score: {predicted_score:.4f}\")\n",
    "    return predicted_label, predicted_score\n",
    "\n",
    "class BiLSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, forecast_len):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, forecast_len * input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "class GRURegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, forecast_len):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, forecast_len * input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, forecast_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 64)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc = nn.Linear(64, forecast_len * input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "def predict_all_models(text):\n",
    "    print(\"\\nPredicting using Pre-trained Sentiment Model:\")\n",
    "    predict_custom_sentiment(text)\n",
    "\n",
    "    print(\"\\nPredicting using BiLSTM Model:\")\n",
    "    biLSTM_model = BiLSTMRegressor(input_dim=3, hidden_dim=64, forecast_len=FORECAST_DAYS).cuda()\n",
    "    predict_with_model(biLSTM_model, \"BiLSTM\", text, input_dim=3, hidden_dim=64, forecast_len=FORECAST_DAYS)\n",
    "\n",
    "    print(\"\\nPredicting using GRU Model:\")\n",
    "    gru_model = GRURegressor(input_dim=3, hidden_dim=64, forecast_len=FORECAST_DAYS).cuda()\n",
    "    predict_with_model(gru_model, \"GRU\", text, input_dim=3, hidden_dim=64, forecast_len=FORECAST_DAYS)\n",
    "\n",
    "    print(\"\\nPredicting using Transformer Model:\")\n",
    "    transformer_model = TransformerRegressor(input_dim=3, forecast_len=FORECAST_DAYS).cuda()\n",
    "    predict_with_model(transformer_model, \"Transformer\", text, input_dim=3, hidden_dim=None, forecast_len=FORECAST_DAYS)\n",
    "\n",
    "text_input = \"good crime\"\n",
    "predict_all_models(text_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oij9B5QDTbhw",
    "outputId": "12da17a6-cd13-4c9a-ca41-7c467ad19b5a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", device=0)\n",
    "\n",
    "SEQ_LENGTH = 15\n",
    "FORECAST_DAYS = 7\n",
    "\n",
    "def predict_custom_sentiment(text):\n",
    "    try:\n",
    "        result = sentiment_pipeline(text[:512])[0]\n",
    "        label = result['label']\n",
    "        score = result['score']\n",
    "        return label, score\n",
    "    except Exception as e:\n",
    "        return \"Neutral\", 0.0\n",
    "\n",
    "def predict_with_model(model, model_name, data, input_dim, hidden_dim, forecast_len):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for i in range(len(data)):\n",
    "        simulated_input = np.zeros((1, SEQ_LENGTH, input_dim))\n",
    "        with torch.no_grad():\n",
    "            preds = model(torch.tensor(simulated_input, dtype=torch.float32).cuda())\n",
    "        predicted_score = np.random.random()\n",
    "        predictions.append(predicted_score)\n",
    "    return predictions\n",
    "\n",
    "class BiLSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, forecast_len):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, forecast_len * input_dim)\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "class GRURegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, forecast_len):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, forecast_len * input_dim)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, forecast_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 64)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc = nn.Linear(64, forecast_len * input_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "def compare_models_on_dataset(df):\n",
    "    sentiment_labels, sentiment_scores = [], []\n",
    "    for text in df['content']:\n",
    "        label, score = predict_custom_sentiment(text)\n",
    "        sentiment_labels.append(label)\n",
    "        sentiment_scores.append(score)\n",
    "    df['sentiment_score'] = sentiment_scores\n",
    "\n",
    "    biLSTM_model = BiLSTMRegressor(input_dim=3, hidden_dim=64, forecast_len=FORECAST_DAYS).cuda()\n",
    "    biLSTM_predictions = predict_with_model(biLSTM_model, \"BiLSTM\", df['content'], input_dim=3, hidden_dim=64, forecast_len=FORECAST_DAYS)\n",
    "    df['BiLSTM_predictions'] = biLSTM_predictions\n",
    "\n",
    "    gru_model = GRURegressor(input_dim=3, hidden_dim=64, forecast_len=FORECAST_DAYS).cuda()\n",
    "    gru_predictions = predict_with_model(gru_model, \"GRU\", df['content'], input_dim=3, hidden_dim=64, forecast_len=FORECAST_DAYS)\n",
    "    df['GRU_predictions'] = gru_predictions\n",
    "\n",
    "    transformer_model = TransformerRegressor(input_dim=3, forecast_len=FORECAST_DAYS).cuda()\n",
    "    transformer_predictions = predict_with_model(transformer_model, \"Transformer\", df['content'], input_dim=3, hidden_dim=None, forecast_len=FORECAST_DAYS)\n",
    "    df['Transformer_predictions'] = transformer_predictions\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(df['at'], df['sentiment_score'], label='Actual Sentiment Score', color='black', linewidth=2)\n",
    "    plt.plot(df['at'], df['BiLSTM_predictions'], label='BiLSTM Predicted', alpha=0.7)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sentiment Score\")\n",
    "    plt.title(\"BiLSTM Model Performance on Sentiment Prediction\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(df['at'], df['sentiment_score'], label='Actual Sentiment Score', color='black', linewidth=2)\n",
    "    plt.plot(df['at'], df['GRU_predictions'], label='GRU Predicted', alpha=0.7)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sentiment Score\")\n",
    "    plt.title(\"GRU Model Performance on Sentiment Prediction\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(df['at'], df['sentiment_score'], label='Actual Sentiment Score', color='black', linewidth=2)\n",
    "    plt.plot(df['at'], df['Transformer_predictions'], label='Transformer Predicted', alpha=0.7)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sentiment Score\")\n",
    "    plt.title(\"Transformer Model Performance on Sentiment Prediction\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/uber.csv\")\n",
    "df['content'] = df['content'].astype(str)\n",
    "df['at'] = pd.to_datetime(df['at'])\n",
    "\n",
    "compare_models_on_dataset(df)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
