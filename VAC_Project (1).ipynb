{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8584,
     "status": "ok",
     "timestamp": 1748004344328,
     "user": {
      "displayName": "2024 18011",
      "userId": "11565921685844415758"
     },
     "user_tz": -330
    },
    "id": "oU9C8QbEGm2b",
    "outputId": "8c62b27d-686f-4430-e836-c4c5ecd2eeb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q gradio langchain langchain-nvidia-ai-endpoints faiss-cpu langchain-community \"unstructured[pdf]\" pdfminer.six transformers tavily-python requests beautifulsoup4 -q streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "00c42bfcea914789a6642b0a54adfc0e",
      "5c14f836e0424690aecdd935b9fa0c23",
      "4616de632f7d46b1aaf8eec97213adee",
      "4373e0e421474c3e9f80ee5accb2ccc4",
      "93d33b73a1ca43b4a3257347c12427d5",
      "9032e417de374f8594e606649a6e3e62",
      "bd6b967a9f5c46fba7ec6a7bc5786d63",
      "fc863a77d9f742678c1b785b5c2fae93",
      "b6c42ca2271f47eeb81ae9a21bce6f85",
      "13b1afc0032441a28ce8d96b9a991645",
      "62ae6e2bb4ef443cbf95cd47d7cfa4ae",
      "f00ec111f3e04ff2b731b13d7de07b83",
      "851b0cfe805143c8a7e5e1cb70de8e78",
      "ad8ba75568d24c01aedbcea00961afe1",
      "5bca5f5400fe4dc0a26a72ce764d16ed",
      "c6fd891f51054b76bff5235cd78b9d46",
      "d3cd141946f74909ad8f1f45dc23e452",
      "fcb49bd4e1474ef2b044fdaa947b0ca0",
      "7c95c062c33f4d929ea99a927fea920a",
      "833f1d8b1b144e108c194e33a1f8fb6d",
      "e8eb5888b6be44628476c5a84f5c2ffd",
      "a1f37727bd8d438f92cffd66054dc8c0",
      "ff8121b6464b48fea46018695fe7dc0f",
      "1a7cc9efbcea474cb29078d7010b4d88",
      "a12e13bfe7e0456386206907722c3566",
      "a61dbb1768b54041b9966869a9ff3475",
      "0c356b398b05498ca801fc2246181047",
      "1489897c73ea4881a8bf4f6f06f9f318",
      "56e6565953d24e419201c35dc2d6eada",
      "d782935df66440e39a96d7d2943d9855",
      "29f8e1c1ba734ba8acaff86fcfc6f285",
      "3665f38eb61e45258bafb15c1f8205e2",
      "0cd3ea926b934bffbc3fe1fca971e94a",
      "cdcc0bef0136461f99dbdad22691a804",
      "8fa7dcff999041c486c7497f302c3f6e",
      "c2dbd3eefdfe4d138f1c14662f77cba8",
      "c88e4029ab7e4fcba730ff0558901f63",
      "4f5226c7719e4369b5a464ec7f6ee457",
      "bad1f278028a4f7bb641a7da9fa0cf7b",
      "70e122dc42aa4706a6bfaf7017cb03d8",
      "a29f871491c947cd8b62fef018c0023b",
      "3b3d9ad49f4143e79fafa160b27da94f",
      "58036ebedf334e62839634c9fd38d82a",
      "cf894c3d1a6f4bfa8fdf744d32ad99b1"
     ]
    },
    "executionInfo": {
     "elapsed": 28245,
     "status": "ok",
     "timestamp": 1747969899089,
     "user": {
      "displayName": "2024 18011",
      "userId": "11565921685844415758"
     },
     "user_tz": -330
    },
    "id": "3mFr4mz9921n",
    "outputId": "8326c21f-ffcc-4752-ae31-702ae117df35"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c42bfcea914789a6642b0a54adfc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00ec111f3e04ff2b731b13d7de07b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8121b6464b48fea46018695fe7dc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcc0bef0136461f99dbdad22691a804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_text_splitters.base:Created a chunk of size 816, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 412, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2143, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2091, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2285, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2121, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 400, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 307, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 371, which is longer than the specified 300\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Loaded 4 documents\n",
      "ğŸ” Created 464 chunks\n",
      "âœ… Filtered to 462 safe chunks under 512 tokens\n",
      "âœ… Rebuilt vector store.\n",
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://3ededfa6b55dde44ac.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3ededfa6b55dde44ac.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ”§ 1. Installing Dependencies\n",
    "# !pip install -q gradio langchain langchain-nvidia-ai-endpoints faiss-cpu langchain-community \"unstructured[pdf]\" pdfminer.six transformers\n",
    "\n",
    "# ğŸ” 2. Load NVIDIA API Key\n",
    "from google.colab import userdata\n",
    "api_key = userdata.get(\"NVIDIA_API_KEY\")\n",
    "\n",
    "# ğŸ— 3. Imports & Config\n",
    "import os\n",
    "import pickle\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DOCS_DIR = \"./uploaded_docs\"\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "VECTORSTORE_PATH = \"vectorstore.pkl\"\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "# ğŸ”§ Accurate Token Counter using HF tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, truncation=False))\n",
    "\n",
    "# ğŸ“‚ 4. Load Documents\n",
    "loader = DirectoryLoader(DOCS_DIR)\n",
    "raw_documents = loader.load()\n",
    "print(f\"ğŸ“„ Loaded {len(raw_documents)} documents\")\n",
    "\n",
    "# ğŸ“ 5. Split and Filter Chunks\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=100, separator='\\n')\n",
    "documents = splitter.split_documents(raw_documents)\n",
    "print(f\"ğŸ” Created {len(documents)} chunks\")\n",
    "\n",
    "safe_documents = [doc for doc in documents if count_tokens(doc.page_content) <= MAX_TOKENS]\n",
    "print(f\"âœ… Filtered to {len(safe_documents)} safe chunks under {MAX_TOKENS} tokens\")\n",
    "\n",
    "# ğŸ¤– 6. Embedding Model\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\", model_type=\"passage\", api_key=api_key)\n",
    "\n",
    "# ğŸ’¾ 7. Build FAISS Vectorstore Every Time\n",
    "vectorstore = FAISS.from_documents(safe_documents, embedder)\n",
    "with open(VECTORSTORE_PATH, \"wb\") as f:\n",
    "    pickle.dump(vectorstore, f)\n",
    "print(\"âœ… Rebuilt vector store.\")\n",
    "\n",
    "# ğŸ§  8. RAG Chain Setup\n",
    "llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\", api_key=api_key)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant named Envie. Use any provided context.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "# ğŸ” 9. RAG Query Function\n",
    "def ask(question: str, k_retrieval: int = 4):\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    docs = retriever.invoke(question, k=k_retrieval)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    prompt_input = f\"Context:\\n{context}\\n\\nQuestion: {question}\" if docs else f\"Question: {question}\"\n",
    "    return \"\".join(chunk for chunk in chain.stream({\"input\": prompt_input}))\n",
    "\n",
    "# ğŸ“¤ 10. Upload + Rebuild Helpers\n",
    "def upload_and_rebuild(file):\n",
    "    import shutil\n",
    "    if file:\n",
    "        shutil.move(file.name, os.path.join(DOCS_DIR, os.path.basename(file.name)))\n",
    "    return \"ğŸ“ Uploaded. Click 'Rebuild Vectorstore'.\"\n",
    "\n",
    "def rebuild_vectorstore():\n",
    "    loader = DirectoryLoader(DOCS_DIR)\n",
    "    raw_documents = loader.load()\n",
    "    documents = CharacterTextSplitter(chunk_size=300, chunk_overlap=100).split_documents(raw_documents)\n",
    "    safe_documents = [doc for doc in documents if count_tokens(doc.page_content) <= MAX_TOKENS]\n",
    "\n",
    "    global vectorstore\n",
    "    vectorstore = FAISS.from_documents(safe_documents, embedder)\n",
    "    with open(VECTORSTORE_PATH, \"wb\") as f:\n",
    "        pickle.dump(vectorstore, f)\n",
    "    return f\"âœ… Vectorstore rebuilt with {len(safe_documents)} chunks.\"\n",
    "\n",
    "# ğŸ–¼ï¸ 11. Gradio Interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ğŸš€ RAG Assistant - Ask Questions About Your Documents\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        upload_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
    "\n",
    "    upload_btn = gr.Button(\"Upload\")\n",
    "    upload_btn.click(upload_and_rebuild, inputs=file_input, outputs=upload_status)\n",
    "\n",
    "    rebuild_btn = gr.Button(\"Rebuild Vectorstore\")\n",
    "    rebuild_output = gr.Textbox(label=\"Rebuild Output\", interactive=False)\n",
    "    rebuild_btn.click(rebuild_vectorstore, outputs=rebuild_output)\n",
    "\n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(label=\"Enter your question\")\n",
    "        output_display = gr.Textbox(label=\"Answer\", lines=10)\n",
    "\n",
    "    ask_button = gr.Button(\"Submit\")\n",
    "    ask_button.click(fn=ask, inputs=question_input, outputs=output_display)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 10867,
     "status": "ok",
     "timestamp": 1747975607321,
     "user": {
      "displayName": "2024 18011",
      "userId": "11565921685844415758"
     },
     "user_tz": -330
    },
    "id": "u9jzNrkOXOzS",
    "outputId": "951ec2a4-6a1e-47cb-c435-5b447eca7da5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 510, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 482, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 505, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 345, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 617, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 309, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 335, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 342, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 348, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 337, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 447, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 359, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 310, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 322, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 339, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 362, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 328, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 350, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 439, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 310, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 343, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 510, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 482, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 505, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 345, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 617, which is longer than the specified 300\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 309, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://ce80da13005b8177f2.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ce80da13005b8177f2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ”§ 1. Installing Dependencies\n",
    "# !pip install -q gradio langchain langchain-nvidia-ai-endpoints faiss-cpu langchain-community \"unstructured[pdf]\" pdfminer.six transformers requests beautifulsoup4\n",
    "\n",
    "# ğŸ” 2. Load API Keys\n",
    "from google.colab import userdata\n",
    "api_key = userdata.get(\"NVIDIA_API_KEY\")\n",
    "\n",
    "# ğŸ— 3. Imports & Config\n",
    "import os\n",
    "import pickle\n",
    "import gradio as gr\n",
    "import shutil\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DOCS_DIR = \"./uploaded_docs\"\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "VECTORSTORE_PATH = \"vectorstore.pkl\"\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "# ğŸ”§ 4. Token Counter\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, truncation=False))\n",
    "\n",
    "# ğŸ“‚ 5. Load and Split Docs\n",
    "loader = DirectoryLoader(DOCS_DIR)\n",
    "raw_documents = loader.load()\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=100, separator=\"\\n\")\n",
    "documents = splitter.split_documents(raw_documents)\n",
    "safe_documents = [doc for doc in documents if count_tokens(doc.page_content) <= MAX_TOKENS]\n",
    "\n",
    "# ğŸ¤– 6. Embedding & Vectorstore\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\", model_type=\"passage\", api_key=api_key)\n",
    "vectorstore = FAISS.from_documents(safe_documents, embedder)\n",
    "with open(VECTORSTORE_PATH, \"wb\") as f:\n",
    "    pickle.dump(vectorstore, f)\n",
    "\n",
    "# ğŸ§  7. LLM Agent (Envie)\n",
    "llm_envie = ChatNVIDIA(model=\"meta/llama3-70b-instruct\", api_key=api_key)\n",
    "template_envie = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant named Envie. Use the context to answer the user's question.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "envie_chain = template_envie | llm_envie | StrOutputParser()\n",
    "\n",
    "# ğŸ¤– 8. Book Recommender Agent\n",
    "llm_books = ChatNVIDIA(model=\"meta/llama3-70b-instruct\", api_key=api_key)\n",
    "template_books = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a book expert. Extract the main topic from the user's question.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "book_topic_chain = template_books | llm_books | StrOutputParser()\n",
    "\n",
    "def fetch_books_by_topic(topic):\n",
    "    import urllib.parse\n",
    "    search_query = f\"best books on {topic}\"\n",
    "    query = urllib.parse.quote_plus(search_query)\n",
    "    url = f\"https://html.duckduckgo.com/html/?q={query}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    results = []\n",
    "    for link in soup.select(\"a.result__a\")[:5]:\n",
    "        title = link.get_text(strip=True)\n",
    "        href = link.get(\"href\")\n",
    "        results.append(f\"- [{title}]({href})\")\n",
    "\n",
    "    return \"\\n\".join(results) if results else \"âŒ No books found via search.\"\n",
    "\n",
    "# ğŸ” 9. RAG + Agent Query Function (updated with markdown output)\n",
    "def ask(question: str, k_retrieval: int = 4):\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    docs = retriever.invoke(question, k=k_retrieval)\n",
    "\n",
    "    if docs:\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        prompt_input = f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
    "        context_msg = \"ğŸ“š Using local document context.\"\n",
    "    else:\n",
    "        prompt_input = f\"Question: {question}\"\n",
    "        context_msg = \"ğŸŒ No local documents found, using general LLM response.\"\n",
    "\n",
    "    answer = \"\".join(chunk for chunk in envie_chain.stream({\"input\": prompt_input}))\n",
    "\n",
    "    # âœ³ï¸ Book Agent Call\n",
    "    topic = book_topic_chain.invoke({\"question\": question}).strip()\n",
    "    books = fetch_books_by_topic(topic)\n",
    "\n",
    "    output_md = f\"\"\"\n",
    "### âœ¨ AI Insight & Recommended Books ğŸ“š\n",
    "\n",
    "{answer.strip()}\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ” **Main Topic:**\n",
    "**{topic}**\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“š **Top Book Recommendations:**\n",
    "\n",
    "{books}\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”— **Tips:**\n",
    "- Click on the book titles above to learn more.\n",
    "- Combine reading with practice for best results.\n",
    "- Ask more questions to explore deeper!\n",
    "\n",
    "---\n",
    "\n",
    "*{context_msg}*\n",
    "\"\"\"\n",
    "    return output_md\n",
    "\n",
    "# ğŸ“¤ 10. Upload + Rebuild Vectorstore\n",
    "def upload_and_rebuild(file):\n",
    "    if file:\n",
    "        shutil.move(file.name, os.path.join(DOCS_DIR, os.path.basename(file.name)))\n",
    "    return \"ğŸ“ Uploaded. Click 'Rebuild Vectorstore'.\"\n",
    "\n",
    "def rebuild_vectorstore():\n",
    "    loader = DirectoryLoader(DOCS_DIR)\n",
    "    raw_documents = loader.load()\n",
    "    documents = CharacterTextSplitter(chunk_size=300, chunk_overlap=100).split_documents(raw_documents)\n",
    "    safe_documents = [doc for doc in documents if count_tokens(doc.page_content) <= MAX_TOKENS]\n",
    "\n",
    "    global vectorstore\n",
    "    vectorstore = FAISS.from_documents(safe_documents, embedder)\n",
    "    with open(VECTORSTORE_PATH, \"wb\") as f:\n",
    "        pickle.dump(vectorstore, f)\n",
    "    return f\"âœ… Vectorstore rebuilt with {len(safe_documents)} chunks.\"\n",
    "\n",
    "# ğŸ–¼ï¸ 11. Gradio Interface (updated output component to Markdown)\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ğŸ¤– Envie Multi-Agent RAG + ğŸ“š Book Recommender\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        upload_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
    "    upload_btn = gr.Button(\"Upload\")\n",
    "    upload_btn.click(upload_and_rebuild, inputs=file_input, outputs=upload_status)\n",
    "\n",
    "    rebuild_btn = gr.Button(\"Rebuild Vectorstore\")\n",
    "    rebuild_output = gr.Textbox(label=\"Rebuild Output\", interactive=False)\n",
    "    rebuild_btn.click(rebuild_vectorstore, outputs=rebuild_output)\n",
    "\n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(label=\"Ask a Question (any topic)\")\n",
    "        output_display = gr.Markdown(label=\"Answer + Books\")  # changed to Markdown for rich output\n",
    "\n",
    "    ask_button = gr.Button(\"Submit\")\n",
    "    ask_button.click(fn=ask, inputs=question_input, outputs=output_display)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9423,
     "status": "ok",
     "timestamp": 1748005281797,
     "user": {
      "displayName": "2024 18011",
      "userId": "11565921685844415758"
     },
     "user_tz": -330
    },
    "id": "KgbnJYd1xL0o",
    "outputId": "0b2108ce-2c34-4b74-f6ab-07c9c8c2e030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://cc03db939ea9676ec6.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cc03db939ea9676ec6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ”§ Install dependencies\n",
    "!pip install -q gradio langchain langchain-nvidia-ai-endpoints faiss-cpu langchain-community \"unstructured[pdf]\" pdfminer.six transformers requests beautifulsoup4\n",
    "\n",
    "# ğŸ” Load API Key\n",
    "from google.colab import userdata\n",
    "api_key = userdata.get(\"NVIDIA_API_KEY\")\n",
    "\n",
    "# ğŸ“š Imports\n",
    "import os, pickle, shutil, requests\n",
    "from bs4 import BeautifulSoup\n",
    "import gradio as gr\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ğŸ“ Constants\n",
    "DOCS_DIR = \"./uploaded_docs\"\n",
    "VECTORSTORE_PATH = \"vectorstore.pkl\"\n",
    "MAX_TOKENS = 512\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "\n",
    "# ğŸ”¢ Token Counter\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, truncation=False))\n",
    "\n",
    "# ğŸ¤– AI Agents\n",
    "llm_envie = ChatNVIDIA(model=\"meta/llama3-70b-instruct\", api_key=api_key)\n",
    "envie_chain = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant named Envie. Use the context to answer the user's question.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "]) | llm_envie | StrOutputParser()\n",
    "\n",
    "llm_books = ChatNVIDIA(model=\"meta/llama3-70b-instruct\", api_key=api_key)\n",
    "book_topic_chain = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a book expert. Extract the main topic from the user's question.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "]) | llm_books | StrOutputParser()\n",
    "\n",
    "# ğŸ§  Embeddings\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\", model_type=\"passage\", api_key=api_key)\n",
    "\n",
    "# ğŸ—‚ï¸ Load & Process Docs\n",
    "def load_docs():\n",
    "    loader = DirectoryLoader(DOCS_DIR)\n",
    "    raw_docs = loader.load()\n",
    "    splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=100, separator=\"\\n\")\n",
    "    docs = splitter.split_documents(raw_docs)\n",
    "    return [doc for doc in docs if count_tokens(doc.page_content) <= MAX_TOKENS]\n",
    "\n",
    "def build_vectorstore():\n",
    "    docs = load_docs()\n",
    "    store = FAISS.from_documents(docs, embedder)\n",
    "    with open(VECTORSTORE_PATH, \"wb\") as f:\n",
    "        pickle.dump(store, f)\n",
    "    return store, len(docs)\n",
    "\n",
    "# ğŸ“¥ Upload & Rebuild\n",
    "def upload_and_rebuild(file):\n",
    "    if file:\n",
    "        shutil.move(file.name, os.path.join(DOCS_DIR, os.path.basename(file.name)))\n",
    "    return \"ğŸ“ Uploaded. Click 'Rebuild Vectorstore'.\"\n",
    "\n",
    "def rebuild():\n",
    "    global vectorstore\n",
    "    vectorstore, chunk_count = build_vectorstore()\n",
    "    return f\"âœ… Vectorstore rebuilt with {chunk_count} chunks.\"\n",
    "\n",
    "# ğŸ” Fetch Books\n",
    "def fetch_books(topic):\n",
    "    import urllib.parse\n",
    "    query = urllib.parse.quote_plus(f\"best books on {topic}\")\n",
    "    url = f\"https://html.duckduckgo.com/html/?q={query}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers).text, \"html.parser\")\n",
    "    return \"\\n\".join(\n",
    "        f\"- [{link.get_text(strip=True)}]({link.get('href')})\"\n",
    "        for link in soup.select(\"a.result__a\")[:5]\n",
    "    ) or \"âŒ No books found.\"\n",
    "\n",
    "# ğŸ’¬ Ask Function\n",
    "def ask(question):\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    docs = retriever.invoke(question, k=4)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs) if docs else \"\"\n",
    "    prompt_input = f\"Context:\\n{context}\\n\\nQuestion: {question}\" if docs else f\"Question: {question}\"\n",
    "    context_msg = \"ğŸ“š Used document context.\" if docs else \"ğŸŒ No context found.\"\n",
    "\n",
    "    answer = \"\".join(chunk for chunk in envie_chain.stream({\"input\": prompt_input}))\n",
    "    topic = book_topic_chain.invoke({\"question\": question}).strip()\n",
    "    books = fetch_books(topic)\n",
    "\n",
    "    return f\"\"\"\n",
    "### âœ¨ AI Insight & Recommendations\n",
    "\n",
    "{answer.strip()}\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ” Main Topic:** {topic}\n",
    "\n",
    "**ğŸ“š Book Recommendations:**\n",
    "\n",
    "{books}\n",
    "\n",
    "---\n",
    "\n",
    "{context_msg}\n",
    "\"\"\"\n",
    "\n",
    "# ğŸŒ Gradio App\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ğŸ¤– Envie Multi-Agent RAG + ğŸ“š Book Recommender\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        upload_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
    "    gr.Button(\"Upload\").click(upload_and_rebuild, inputs=file_input, outputs=upload_status)\n",
    "\n",
    "    rebuild_output = gr.Textbox(label=\"Rebuild Output\", interactive=False)\n",
    "    gr.Button(\"Rebuild Vectorstore\").click(rebuild, outputs=rebuild_output)\n",
    "\n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(label=\"Ask a question\")\n",
    "        output_display = gr.Markdown(label=\"Response\")\n",
    "\n",
    "    gr.Button(\"Submit\").click(ask, inputs=question_input, outputs=output_display)\n",
    "\n",
    "# ğŸš€ Launch in notebook\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMz96IWPmWCJjUl02ciJCII",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
